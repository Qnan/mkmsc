\section{Метод инкрементальной редукции\label{s:increduction}}
	Редукция зачастую осуществляется относительно набора полиномов, не являющегося базисом Грёбнера. В этом случае результат неединственен и может зависеть от используемого алгоритма редукции. Однако, можно говорить о нормальной форме полинома в более общем смысле, если зафиксировать метод редукции. В данном разделе, говоря о нормальной форме, мы будем иметь в виду именно этот обощенный вариант. Также следует обратить внимание на тот факт, что мы используем два различных метода редукции, первый из которых может быть выбран произвольно, а второй -- о котором пойдет речь -- строится на основе первого.
	
	\subsection{Задача редукции\label{ss:reductionstatement}}
	Задача редукции полинома $p$ относительно набора полиномов $H$ суть задача поиска остатка от деления этого полинома на них. Простейший подход \cite{lib:cox} к его нахождению заключается в последовательном вычитании $h\in H$ из $p$ с некоторыми множителями. Чтобы гарантировать, что редукция в какой-то момент завершится, обычно фиксируют некоторое \textit{допустимое мономиальное упорядочение} и используют на каждом шаге один из полиномов, чей старший член (в смысле указанного упорядочения) делит старший член рассматриваемого полинома, а в качестве множителя используется их частное.
	
	Когда же вычитание выполнить нельзя, старший член полинома перемещается в остаток, и процедура повторяется до тех пор, пока $p$ не окажется равным нулю. 
	
	[простой алгоритм редукции: псевдокод]
	
	[Оценка сложности]
	
	На практике, низкая эффективность алгоритма редукции связана с ростом количества промежуточных членов. В процессе редукции количество членов в полиноме может интенсивно расти. Рассмотрим, в качестве примера, набор 
	$$G=\{x_1^2-(1 + \sum_{i=2}^m x_i)\}\cup\{x_i|i=2..m\}.$$
Теперь редуцируем $x^{2n}$ относительно $G$. Если применять первое правило, пока это возможно, мы получим $(1+\sum_{i=2}^m x_i)^n$, содержащий $C^{n+m-2}_{n}$ членов, который в конечном счете будет редуцирован к 0. Данный пример, разумеется, является искусственным. На практике число промежуточных членов обычно не растет столь интенсивно, однако существенное влияние этого фактора на эффективность алгоритма редукции установлено.

[картинка с картой мономов в (x,y)]

	Метод инкрементальной редукции направлен на сокращение количества промежуточных членов. Приведем пример
	[пример последовательной редукци]
	
	[последовательно не всегда быстрее]

	\subsection{Описание метода\label{ss:inrnormdesc}}
	Метод инкрементальной редукции основан на двух свойствах нормальных форм (\ref{eq:nf:props:sum},~\ref{eq:nf:props:prod}). Первое из них -- линейность оператора нормализации. Второе заключается в том, что нормальная форма произведения двух полиномов равна нормальной форме произведения их нормальных форм.
	
\begin{equation}
	\label{eq:nf:props:sum}
	N(\lambda p + \mu q)=\lambda N(p) + \mu N(q)
\end{equation}
\begin{equation}
	\label{eq:nf:props:prod}
	N(p\cdot q)=N(N(p)\cdot N(q))
\end{equation}
	
	Используя эти правила, можно определить пространство нормальных форм по заданному базису. Легко видеть, что оно изоморфно факторалгебре по идеалу, заданному рассматриваемым базисом. 
	
\begin{equation}
	\label{eq:nf:nfspace:sum}
	\lambda p +_n \mu q=\lambda p + \mu q
\end{equation}
\begin{equation}
	\label{eq:nf:nfspace:prod}
	p \cdot_n q=N(p\cdot q)
\end{equation}	
	
	Итак, метод инкрементальной редукции представляет собой операцию подстановки -- или \textit{замены переменных} -- в факторалгебре. Следует заметить, что мы фактически не изменяем имена переменных, поэтому подстановку можно рассматривать как тождественную, но не следует забывать, что она выполняется в другом пространстве.
	
	Следует заметить, что в определении операции умножения в пространстве нормальных форм присутствует оператор нормализации. Ему соответствует некоторый алгоритм редукции, не совпадающий с методом инкрементальной редукции, так как в случае их совпадения определение последний оказался бы определен рекурсивно. Мы обсудим этот оператор подробнее в секции \ref{sss:N}.
	
	Итак, метод инкрементальной редукции можно кратко описать следующим образом
	\begin{itemize}
		\item вычислить нормальные формы мономов нижнего уровня
		\item выполнить операции согласно схемы, используя (\ref{eq:nf:nfspace:sum},~\ref{eq:nf:nfspace:prod})
		\item вернуть результат последней операции в качестве ответа
	\end{itemize}

	\subsubsection{Сложность операций умножения\label{sss:criteriainconsistency}}
	Очевидно, что эффективность метода существенно зависит от используемой схемы, причем аппроксимировать критерий качества в данном случае весьма проблематично -- сложность редукции двух мономов, сигнатуры которых отличаются на единицу, может различаться в произвольное число раз.
	
	[пример?]
	
	С другой стороны, операции сложения и умножения на число имеют, в большинстве случаев, пренебрежимо малую стоимость, по сравнению с операцией умножения. Исключение составляют, например, булевы полиномы в пространствах высокой размерности, для которых характерно очень большое количество членов.
	
	\subsection{Кеширование нормальных форм\label{ss:nfcache}}
В процессе работы алгоритма мы фактически выполняем редукцию лишь для отдельных мономов. При этом эксперименты показывают, что мономы достаточно часто повторяются, что делает невыгодной повторную редукцию. Вместо этого, можно кешировать мономы -- все или только часто встречающиеся, в зависимости от доступного объема памяти -- и не пересчитывать их нормалную форму при повторном запросе.

В данной работе мы реализуем такую схему кеширования (см. \ref{ss:nfcacheimpl}) и оцениваем ее эффективность. Следует заметить, что данная идея не нова -- она реализована, в частности, в \cite{lib:kliskunov}. Есть также определенные основания полагать, что некоторая схожая схема кеширования реализована и в система компьютерной алгобры Singular (см. \ref{s:results}). 

	\subsection{Возможные улучшения\label{ss:online}}
		Метод инкрементальной редукции демонстрирует высокую эффективность в наших экспериментах. Однако, с тем, чтобы глубже узнать сильные и слабые стороны, следует рассматривать фактическую сложность выполнения отдельных операций. С одной стороны, это позволит идентифицировать узкие места, и, возможно, рассмотреть потенциальные улучшения этого метода. С другой стороны, такой подход может помочь построить более точный приближенный критерий сложности, с тем чтобы генерировать более эффективные схемы.
		
